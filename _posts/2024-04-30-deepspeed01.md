---
layout: distill
title: Training with DeepSpeed - Basic Concepts
date: 2024-04-30 11:46:00
description: basic concepts behind of DeepSpeed
tags: pre-training code
categories: LLM
# thumbnail: assets/img/9.jpg
# images:
#  compare: true
#  slider: true

toc:
  - name: Concepts of Model Parallelism
    subsections:
    - name: DataParallel (DP)
    - name: TensorParallel (TP)
    - name: Naive Model Parallel (NMP) and PipelineParallel (PP) 
  - name: Memory Consumption in Model Training
  - name: Reference
---


## Concepts of Model Parallelism

There are a few concepts that need to be reviewed: DataParallel (DP), TensorParallel (TP), PipelineParallel (PP).

###  DataParallel (DP)
In DP, the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step.

Consider a simple model with 3 layers, where each layer has 2 params:
```bash
La | Lb | Lc
---|----|---
a0 | b0 | c0
a1 | b1 | c1
```

If we have two GPUs, DP splits the model onto 2 GPUs like so:
```bash
GPU0: 
L0 | L1 | L2
---|----|---
a0 | b0 | c0
a1 | b1 | c1

GPU1:
L0 | L1 | L2
---|----|---
a0 | b0 | c0
a1 | b1 | c1
```
Each GPU holds a model, and with every iteration (step), the batch data is divided into 2 equally sized micro-batches. Each GPU independently calculates the gradients based on the micro-batch data it receives. Then the gradient is averaged by the Allreduce algorithm across all GPUs.


### TensorParallel (TP)

Each tensor is split up into multiple chunks. Each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is also called horizontal parallelism, as the splitting happens on horizontal level.

```bash
GPU0:
L0 | L1 | L2
---|----|---
a0 | b0 | c0

GPU1:
L0 | L1 | L2
---|----|---
a1 | b1 | c1
```

### Naive Model Parallel (NMP) and PipelineParallel (PP) 

In `Naive Model Parallel (MP)`, the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are places on a single gpu. Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch.

```bash
================   =================
|  L0 | L1 | L2 |   | L3 | L4 | L5 |
=================  =================
        GPU0                 GPU1
```


`Pipeline Parallel (PP)` is almost identical to a naive MP, but it solves the GPU `idling problem`, by chunking the incoming batch into micro-batches and artificially creating a pipeline, which allows different GPUs to concurrently participate in the computation process.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Image Source: <a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">HuggingFace Documentation</a> 
</div>


## Memory Consumption in Model Training

During model training, most of the memory is consumed by two things:
- `Model states`, which includes tensors comprising of optimizer states, gradients, and parameters.
- `Activations`, which includes any tensor that is created in the forward pass and is necessary for gradient computation during backward pass.

The state-of-the-art approach to train LLMs on the current generation of NVIDIA GPUs is via mixed precision. 

Assume we train a model with $\Uppsi$ parameters using Adam. This requires to $2\Uppsi + 2\Uppsi + 3*4\Uppsi = 16\Uppsi$ bytes of memory requirement (see our blog [Mixed-precision training in LLM](https://ilampard.github.io/blog/2024/mixed-precision/) for details).



## Introduction to ZeRO

Challege to previous parallelism:
- DP: it does not help reduce memory footprint per device: a model with more than 1 billion parameters runs out of memory even on GPUs with 32GB of memory.
- TP: it does not scale efficiently beyond a single node due to fine-grained computation and expensive communication.

        
## Reference

[Model Parallelism](https://huggingface.co/docs/transformers/v4.15.0/parallelism)

[Transformer Math](https://blog.eleuther.ai/transformer-math/) 

[ZeRO & DeepSpeed: New system optimizations enable training models with over 100 billion parameters](https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/)

[图解大模型训练之：流水线并行（Pipeline Parallelism），以Gpipe为例](https://zhuanlan.zhihu.com/p/613196255)


