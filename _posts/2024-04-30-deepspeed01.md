---
layout: distill
title: Training with DeepSpeed - Basic Concepts
date: 2024-04-30 11:46:00
description: basic concepts behind of DeepSpeed
tags: pre-training code
categories: LLM
# thumbnail: assets/img/9.jpg
# images:
#  compare: true
#  slider: true

toc:
  - name: Concepts of Model Parallelism
    subsections:
    - name: DataParallel (DP)
    - name: TensorParallel (TP)
    - name: PipelineParallel (PP)
  - name: Memory Consumption in Model Training
  - name: Reference
---


## Concepts of Model Parallelism

There are a few concepts that need to be reviewed: DataParallel (DP), TensorParallel (TP), PipelineParallel (PP).

###  DataParallel (DP)
In DP, the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step.

Consider a simple model with 3 layers, where each layer has 2 params:
```bash
La | Lb | Lc
---|----|---
a0 | b0 | c0
a1 | b1 | c1
```

If we have two GPUs, DP splits the model onto 2 GPUs like so:
```bash
GPU0: 
L0 | L1 | L2
---|----|---
a0 | b0 | c0
a1 | b1 | c1

GPU1:
L0 | L1 | L2
---|----|---
a0 | b0 | c0
a1 | b1 | c1
```
Each GPU holds a model, and with every iteration (step), the batch data is divided into 2 equally sized micro-batches. Each GPU independently calculates the gradients based on the micro-batch data it receives. Then the gradient is averaged by the Allreduce algorithm across all GPUs.


### TensorParallel (TP)

Each tensor is split up into multiple chunks. Each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is also called horizontal parallelism, as the splitting happens on horizontal level.

```bash
GPU0:
L0 | L1 | L2
---|----|---
a0 | b0 | c0

GPU1:
L0 | L1 | L2
---|----|---
a1 | b1 | c1
```

### PipelineParallel (PP) 

In PP, the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are places on a single gpu. Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch.

```bash
================   =================
|  L0 | L1 | L2 |   | L3 | L4 | L5 |
=================  =================
        GPU0                 GPU1
```


## Memory Consumption in Model Training

During model training, most of the memory is consumed by two things:
- `Model states`, which includes tensors comprising of optimizer states, gradients, and parameters.
- `Activations`, which includes any tensor that is created in the forward pass and is necessary for gradient computation during backward pass.

The state-of-the-art approach to train LLMs on the current generation of NVIDIA GPUs is via mixed precision. 

Assume we train a model with $\Uppsi$ parameters using Adam. This requires to $2 \Uppsi + 2\Uppsi + 3*4\Uppsi = 16\Uppsi$ bytes of memory requirement (see our blog [Mixed-precision training in LLM](https://ilampard.github.io/blog/2024/mixed-precision/) for details).

        
## Reference

[Model Parallelism](https://huggingface.co/docs/transformers/v4.15.0/parallelism)
 

<!--

This is an example post with advanced image components.

## Image Slider

This is a simple image slider. It uses the [Swiper](https://swiperjs.com/) library. Check the [examples page](https://swiperjs.com/demos) for more information of what you can achieve with it.

<swiper-container keyboard="true" navigation="true" pagination="true" pagination-clickable="true" pagination-dynamic-bullets="true" rewind="true">
  <swiper-slide>{% include figure.liquid loading="eager" path="assets/img/9.jpg" class="img-fluid rounded z-depth-1" %}</swiper-slide>
  <swiper-slide>{% include figure.liquid loading="eager" path="assets/img/7.jpg" class="img-fluid rounded z-depth-1" %}</swiper-slide>
  <swiper-slide>{% include figure.liquid loading="eager" path="assets/img/8.jpg" class="img-fluid rounded z-depth-1" %}</swiper-slide>
  <swiper-slide>{% include figure.liquid loading="eager" path="assets/img/10.jpg" class="img-fluid rounded z-depth-1" %}</swiper-slide>
  <swiper-slide>{% include figure.liquid loading="eager" path="assets/img/12.jpg" class="img-fluid rounded z-depth-1" %}</swiper-slide>
</swiper-container>

## Image Comparison Slider

This is a simple image comparison slider. It uses the [img-comparison-slider](https://img-comparison-slider.sneas.io/) library. Check the [examples page](https://img-comparison-slider.sneas.io/examples.html) for more information of what you can achieve with it.

<img-comparison-slider>
  {% include figure.liquid path="assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" slot="first" %}
  {% include figure.liquid path="assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" slot="second" %}
</img-comparison-slider>


-->
