<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Basic and Advanced Techniques on RAG | Siqiao Xue </title> <meta name="author" content="Siqiao Xue"> <meta name="description" content="detailed techniques on RAG, with LangChain code examples"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ilampard.github.io/blog/2024/techniques-rag/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Basic and Advanced Techniques on RAG",
            "description": "detailed techniques on RAG, with LangChain code examples",
            "published": "April 28, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Siqiao</span> Xue </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/archive/">archive </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Basic and Advanced Techniques on RAG</h1> <p>detailed techniques on RAG, with LangChain code examples</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction-to-rag">Introduction to RAG</a> </div> <ul> <li> <a href="#indexing">Indexing</a> </li> <li> <a href="#embedding-model">Embedding Model</a> </li> <li> <a href="#retriever">Retriever</a> </li> </ul> <div> <a href="#advanced-techniques-in-retrievers">Advanced Techniques in Retrievers</a> </div> <ul> <li> <a href="#query-rewriting">Query Rewriting</a> </li> </ul> <div> <a href="#reference">Reference</a> </div> </nav> </d-contents> <h2 id="introduction-to-rag">Introduction to RAG</h2> <p>Many LLM applications require user-specific data that is not part of the model’s training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In RAG process, external data is retrieved and then passed to the LLM when doing the generation step. We take <a href="https://python.langchain.com/docs/get_started/introduction" rel="external nofollow noopener" target="_blank">Langchain</a> as the codebase to understand this process.</p> <p>RAG typically involves three process: indexing, retrieval and generation.</p> <h3 id="indexing">Indexing</h3> <p>In indexing process, the systems sync documents from external source into a vector store. Assume we load a website as a document.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain_community</span> <span class="n">tiktoken</span> <span class="n">langchain</span><span class="o">-</span><span class="n">openai</span> <span class="n">langchainhub</span> <span class="n">chromadb</span> <span class="n">langchain</span> <span class="n">langchain_text_splitters</span> <span class="n">sentence_transformers</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load blog
</span><span class="kn">import</span> <span class="n">bs4</span>
<span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="n">loader</span> <span class="o">=</span> <span class="nc">WebBaseLoader</span><span class="p">(</span>
    <span class="n">web_paths</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">"</span><span class="p">,),</span>
    <span class="n">bs_kwargs</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
        <span class="n">parse_only</span><span class="o">=</span><span class="n">bs4</span><span class="p">.</span><span class="nc">SoupStrainer</span><span class="p">(</span>
            <span class="n">class_</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">post-content</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">post-title</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">post-header</span><span class="sh">"</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">blog_docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="nf">len</span><span class="p">(</span><span class="n">blog_docs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">page_content</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">43131</span>
</code></pre></div></div> <p>Once loaded, we need to split the long document into smaller chunks that can fit into the model’s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.</p> <p>Base on LangChain’s document, at a high level, text splitters firstly split the text up into small, semantically meaningful chunks (often sentences); then combine these small chunks into a larger chunk until reaching a certain size (as measured by some function). Once reaching that size, the splitter makes that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).</p> <p>By default, it is recommended to use RecursiveCharacterTextSplitter for generic text. It is parameterized by a list of characters (The default list is <code class="language-plaintext highlighter-rouge">["\n\n", "\n", " ", ""]</code>.). It tries to split on them in order until the chunks are small enough. It has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="c1"># Set a really small chunk size, just to show.
</span>    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">length_function</span><span class="o">=</span><span class="nb">len</span><span class="p">,</span>
    <span class="n">is_separator_regex</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Make splits
</span><span class="n">splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">blog_docs</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">LLM Powered Autonomous Agents</span><span class="sh">'</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng</span><span class="sh">'</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">splits</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="mi">611</span>
</code></pre></div></div> <h3 id="embedding-model">Embedding Model</h3> <p>The embedding model creates a vector representation for a piece of text so we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceBgeEmbeddings</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">BAAI/bge-small-en</span><span class="sh">"</span>
<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">device</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">}</span>
<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">normalize_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
<span class="n">hf</span> <span class="o">=</span> <span class="nc">HuggingFaceBgeEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span> <span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span>
<span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emb</span> <span class="o">=</span> <span class="n">hf</span><span class="p">.</span><span class="nf">embed_query</span><span class="p">(</span><span class="sh">"</span><span class="s">hi this is harrison</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">emb</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="mi">384</span>
</code></pre></div></div> <p>We can initialize the vector store object following the following example</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span> 
                                    <span class="n">embedding</span><span class="o">=</span><span class="n">hf</span><span class="p">)</span>
</code></pre></div></div> <h3 id="retriever">Retriever</h3> <p>A retriever is an interface that returns documents given an unstructured query.</p> <p>Retriever vs vectorstore:</p> <ul> <li>A retriever is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them.</li> <li>Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">()</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">get_relevant_documents</span><span class="p">(</span><span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="nc">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">Task decomposition can be done (1) by LLM with simple prompting like </span><span class="sh">"</span><span class="s">Steps for XYZ.</span><span class="se">\\</span><span class="s">n1.</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">What</span><span class="sh">'</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">})]</span>
</code></pre></div></div> <h2 id="advanced-techniques-in-retrievers">Advanced Techniques in Retrievers</h2> <h3 id="query-rewriting">Query Rewriting</h3> <p>The input query can be ambiguous, causing an inevitab gap between the input text and the knowledge that is really needed to query.</p> <p>A straightforward way is to use LLM to generate queries from multiple perspective, a.k.a. multi-query transform.</p> <p>The whole process: for a given user input query, it uses an LLM to generate multiple queries from different perspectives. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. Lastly, it feeds all the retrieved documents and let LLM to generate the answer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># Multi Query - generate queries from different perspectives using a prompt
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are an AI language model assistant. Your task is to generate five
different versions of the given user question to retrieve relevant documents from a vector
database. By generating multiple perspectives on the user question, your goal is to help
the user overcome some of the limitations of the distance-based similarity search.
Provide these alternative questions separated by newlines. Original question: {question}</span><span class="sh">"""</span>
<span class="n">prompt_perspectives</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">OPENAI_API_KEY</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">xxx</span><span class="sh">'</span>

<span class="n">generate_queries</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">prompt_perspectives</span>
    <span class="o">|</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
    <span class="o">|</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div></div> <p>By applying the multi-query approach, we retrieve more documents than the standard approach and run the retrieval process to get an anaswer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.load</span> <span class="kn">import</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span>

<span class="k">def</span> <span class="nf">get_unique_union</span><span class="p">(</span><span class="n">documents</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s"> Unique union of retrieved docs </span><span class="sh">"""</span>
    <span class="c1"># Flatten list of lists, and convert each Document to string
</span>    <span class="n">flattened_docs</span> <span class="o">=</span> <span class="p">[</span><span class="nf">dumps</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">documents</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
    <span class="c1"># Get unique documents
</span>    <span class="n">unique_docs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">flattened_docs</span><span class="p">))</span>
    <span class="c1"># Return
</span>    <span class="k">return</span> <span class="p">[</span><span class="nf">loads</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">unique_docs</span><span class="p">]</span>

<span class="c1"># Retrieve process
</span><span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span>

<span class="c1"># pass each query to the retriever and remove the duplicate docs
</span><span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">generate_queries</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">map</span><span class="p">()</span> <span class="o">|</span> <span class="n">get_unique_union</span>

<span class="c1"># retrieve
</span><span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">6</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># RAG
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the following question based on this context:

{context}

Question: {question}
</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">final_rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retrieval_chain</span><span class="p">,</span> 
     <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nf">itemgetter</span><span class="p">(</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">)}</span> 
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">final_rag_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">involves</span> <span class="n">parsing</span> <span class="n">user</span> <span class="n">requests</span> <span class="n">into</span> <span class="n">multiple</span> <span class="n">tasks</span><span class="p">,</span> <span class="k">with</span> <span class="n">the</span> <span class="n">LLM</span> <span class="n">acting</span> <span class="k">as</span> <span class="n">the</span> <span class="n">brain</span> <span class="n">to</span> <span class="n">organize</span> <span class="ow">and</span> <span class="n">manage</span> <span class="n">these</span> <span class="n">tasks</span><span class="p">.</span>
</code></pre></div></div> <h3 id="rag-fusion">RAG Fusion</h3> <p>How it works</p> <ul> <li>Performs multi query transformation by translating the user’s queries into similar yet distinct through LLM. (same as MultiQueryRetriever)</li> <li>Initialize the vector searches for the original query and its generated similar queries, multiple query generation. (same as MultiQueryRetriever)</li> <li>Combine and refine all the query results using $RRF=\frac{1}{rank+k}$, where $rank$ is the current rank of the documents sorted by distance, and $k$ is a constant smoothing factor that determines the weight given to the existing ranks.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-480.webp 480w,https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-800.webp 800w,https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image Source: [Adrian H. Raudaschl](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1) </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># RAG-Fusion: Related
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are a helpful assistant that generates multiple search queries based on a single input query. </span><span class="se">\n</span><span class="s">
Generate multiple search queries related to: {question} </span><span class="se">\n</span><span class="s">
Output (4 queries):</span><span class="sh">"""</span>
<span class="n">prompt_rag_fusion</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.load</span> <span class="kn">import</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span>

<span class="k">def</span> <span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> Reciprocal_rank_fusion that takes multiple lists of ranked documents 
        and an optional parameter k used in the RRF formula </span><span class="sh">"""</span>
    
    <span class="c1"># Initialize a dictionary to hold fused scores for each unique document
</span>    <span class="n">fused_scores</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Iterate through each list of ranked documents
</span>    <span class="k">for</span> <span class="n">docs</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="c1"># Iterate through each document in the list, with its rank (position in the list)
</span>        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
            <span class="c1"># Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)
</span>            <span class="n">doc_str</span> <span class="o">=</span> <span class="nf">dumps</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
            <span class="c1"># If the document is not yet in the fused_scores dictionary, add it with an initial score of 0
</span>            <span class="k">if</span> <span class="n">doc_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">fused_scores</span><span class="p">:</span>
                <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Retrieve the current score of the document, if any
</span>            <span class="n">previous_score</span> <span class="o">=</span> <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span>
            <span class="c1"># Update the score of the document using the RRF formula: 1 / (rank + k)
</span>            <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>

    <span class="c1"># Sort the documents based on their fused scores in descending order to get the final reranked results
</span>    <span class="n">reranked_results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="nf">loads</span><span class="p">(</span><span class="n">doc</span><span class="p">),</span> <span class="n">score</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">fused_scores</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Return the reranked results as a list of tuples, each containing the document and its fused score
</span>    <span class="k">return</span> <span class="n">reranked_results</span>

<span class="n">retrieval_chain_rag_fusion</span> <span class="o">=</span> <span class="n">generate_queries</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">map</span><span class="p">()</span> <span class="o">|</span> <span class="n">reciprocal_rank_fusion</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain_rag_fusion</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
<span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">6</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># RAG
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the following question based on this context:

{context}

Question: {question}
</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">final_rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retrieval_chain_rag_fusion</span><span class="p">,</span> 
     <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nf">itemgetter</span><span class="p">(</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">)}</span> 
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">final_rag_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">involves</span> <span class="n">breaking</span> <span class="n">down</span> <span class="n">large</span> <span class="n">tasks</span> <span class="n">into</span> <span class="n">smaller</span><span class="p">,</span> <span class="n">manageable</span> <span class="n">subgoals</span><span class="p">.</span>
</code></pre></div></div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://github.com/langchain-ai/rag-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">LangChain - rag from scrach</a></li> <li><a href="https://github.com/Raudaschl/rag-fusion/blob/master/main.py" rel="external nofollow noopener" target="_blank">RAG-Fusion</a></li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Siqiao Xue. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>