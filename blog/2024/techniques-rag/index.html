<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Basic and Advanced Techniques on RAG | Siqiao Xue </title> <meta name="author" content="Siqiao Xue"> <meta name="description" content="detailed techniques on RAG, with LangChain code examples"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ilampard.github.io/blog/2024/techniques-rag/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Basic and Advanced Techniques on RAG",
            "description": "detailed techniques on RAG, with LangChain code examples",
            "published": "April 28, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Siqiao</span> Xue </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/archive/">archive </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Basic and Advanced Techniques on RAG</h1> <p>detailed techniques on RAG, with LangChain code examples</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction-to-rag">Introduction to RAG</a> </div> <ul> <li> <a href="#indexing">Indexing</a> </li> <li> <a href="#embedding-model">Embedding Model</a> </li> <li> <a href="#retriever">Retriever</a> </li> </ul> <div> <a href="#advanced-techniques-in-retrievers">Advanced Techniques in Retrievers</a> </div> <ul> <li> <a href="#query-rewriting">Query Rewriting</a> </li> <li> <a href="#rag-fusion">RAG Fusion</a> </li> <li> <a href="#step-back-prompting">Step Back Prompting</a> </li> <li> <a href="#corrective-rag">Corrective RAG</a> </li> </ul> <div> <a href="#evaluation">Evaluation</a> </div> <div> <a href="#reference">Reference</a> </div> </nav> </d-contents> <h2 id="introduction-to-rag">Introduction to RAG</h2> <p>Many LLM applications require user-specific data that is not part of the model’s training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In RAG process, external data is retrieved and then passed to the LLM when doing the generation step. We take <a href="https://python.langchain.com/docs/get_started/introduction" rel="external nofollow noopener" target="_blank">Langchain</a> as the codebase to understand this process.</p> <p>RAG typically involves three process: indexing, retrieval and generation.</p> <h3 id="indexing">Indexing</h3> <p>In indexing process, the systems sync documents from external source into a vector store. Assume we load a website as a document.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain_community</span> <span class="n">tiktoken</span> <span class="n">langchain</span><span class="o">-</span><span class="n">openai</span> <span class="n">langchainhub</span> <span class="n">chromadb</span> <span class="n">langchain</span> <span class="n">langchain_text_splitters</span> <span class="n">sentence_transformers</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load blog
</span><span class="kn">import</span> <span class="n">bs4</span>
<span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="n">loader</span> <span class="o">=</span> <span class="nc">WebBaseLoader</span><span class="p">(</span>
    <span class="n">web_paths</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">"</span><span class="p">,),</span>
    <span class="n">bs_kwargs</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
        <span class="n">parse_only</span><span class="o">=</span><span class="n">bs4</span><span class="p">.</span><span class="nc">SoupStrainer</span><span class="p">(</span>
            <span class="n">class_</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">post-content</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">post-title</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">post-header</span><span class="sh">"</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">blog_docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="nf">len</span><span class="p">(</span><span class="n">blog_docs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">page_content</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">43131</span>
</code></pre></div></div> <p>Once loaded, we need to split the long document into smaller chunks that can fit into the model’s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.</p> <p>Base on LangChain’s document, at a high level, text splitters firstly split the text up into small, semantically meaningful chunks (often sentences); then combine these small chunks into a larger chunk until reaching a certain size (as measured by some function). Once reaching that size, the splitter makes that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).</p> <p>By default, it is recommended to use RecursiveCharacterTextSplitter for generic text. It is parameterized by a list of characters (The default list is <code class="language-plaintext highlighter-rouge">["\n\n", "\n", " ", ""]</code>.). It tries to split on them in order until the chunks are small enough. It has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="c1"># Set a really small chunk size, just to show.
</span>    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">length_function</span><span class="o">=</span><span class="nb">len</span><span class="p">,</span>
    <span class="n">is_separator_regex</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Make splits
</span><span class="n">splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">blog_docs</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">LLM Powered Autonomous Agents</span><span class="sh">'</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng</span><span class="sh">'</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">splits</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="mi">611</span>
</code></pre></div></div> <h3 id="embedding-model">Embedding Model</h3> <p>The embedding model creates a vector representation for a piece of text so we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceBgeEmbeddings</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">BAAI/bge-small-en</span><span class="sh">"</span>
<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">device</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">}</span>
<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">normalize_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
<span class="n">hf</span> <span class="o">=</span> <span class="nc">HuggingFaceBgeEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span> <span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span>
<span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emb</span> <span class="o">=</span> <span class="n">hf</span><span class="p">.</span><span class="nf">embed_query</span><span class="p">(</span><span class="sh">"</span><span class="s">hi this is harrison</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">emb</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="mi">384</span>
</code></pre></div></div> <p>We can initialize the vector store object following the following example</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span> 
                                    <span class="n">embedding</span><span class="o">=</span><span class="n">hf</span><span class="p">)</span>
</code></pre></div></div> <h3 id="retriever">Retriever</h3> <p>A retriever is an interface that returns documents given an unstructured query.</p> <p>Retriever vs vectorstore:</p> <ul> <li>A retriever is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them.</li> <li>Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">()</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">get_relevant_documents</span><span class="p">(</span><span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="nc">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">Task decomposition can be done (1) by LLM with simple prompting like </span><span class="sh">"</span><span class="s">Steps for XYZ.</span><span class="se">\\</span><span class="s">n1.</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">What</span><span class="sh">'</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">})]</span>
</code></pre></div></div> <h2 id="advanced-techniques-in-retrievers">Advanced Techniques in Retrievers</h2> <h3 id="query-rewriting">Query Rewriting</h3> <p>The input query can be ambiguous, causing an inevitab gap between the input text and the knowledge that is really needed to query.</p> <p>A straightforward way is to use LLM to generate queries from multiple perspective, a.k.a. multi-query transform.</p> <p>The whole process: for a given user input query, it uses an LLM to generate multiple queries from different perspectives. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. Lastly, it feeds all the retrieved documents and let LLM to generate the answer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># Multi Query - generate queries from different perspectives using a prompt
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are an AI language model assistant. Your task is to generate five
different versions of the given user question to retrieve relevant documents from a vector
database. By generating multiple perspectives on the user question, your goal is to help
the user overcome some of the limitations of the distance-based similarity search.
Provide these alternative questions separated by newlines. Original question: {question}</span><span class="sh">"""</span>
<span class="n">prompt_perspectives</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">OPENAI_API_KEY</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">xxx</span><span class="sh">'</span>

<span class="n">generate_queries</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">prompt_perspectives</span>
    <span class="o">|</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
    <span class="o">|</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div></div> <p>By applying the multi-query approach, we retrieve more documents than the standard approach and run the retrieval process to get an anaswer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.load</span> <span class="kn">import</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span>

<span class="k">def</span> <span class="nf">get_unique_union</span><span class="p">(</span><span class="n">documents</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s"> Unique union of retrieved docs </span><span class="sh">"""</span>
    <span class="c1"># Flatten list of lists, and convert each Document to string
</span>    <span class="n">flattened_docs</span> <span class="o">=</span> <span class="p">[</span><span class="nf">dumps</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">documents</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
    <span class="c1"># Get unique documents
</span>    <span class="n">unique_docs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">flattened_docs</span><span class="p">))</span>
    <span class="c1"># Return
</span>    <span class="k">return</span> <span class="p">[</span><span class="nf">loads</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">unique_docs</span><span class="p">]</span>

<span class="c1"># Retrieve process
</span><span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span>

<span class="c1"># pass each query to the retriever and remove the duplicate docs
</span><span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">generate_queries</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">map</span><span class="p">()</span> <span class="o">|</span> <span class="n">get_unique_union</span>

<span class="c1"># retrieve
</span><span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">6</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># RAG
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the following question based on this context:

{context}

Question: {question}
</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">final_rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retrieval_chain</span><span class="p">,</span> 
     <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nf">itemgetter</span><span class="p">(</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">)}</span> 
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">final_rag_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">involves</span> <span class="n">parsing</span> <span class="n">user</span> <span class="n">requests</span> <span class="n">into</span> <span class="n">multiple</span> <span class="n">tasks</span><span class="p">,</span> <span class="k">with</span> <span class="n">the</span> <span class="n">LLM</span> <span class="n">acting</span> <span class="k">as</span> <span class="n">the</span> <span class="n">brain</span> <span class="n">to</span> <span class="n">organize</span> <span class="ow">and</span> <span class="n">manage</span> <span class="n">these</span> <span class="n">tasks</span><span class="p">.</span>
</code></pre></div></div> <h3 id="rag-fusion">RAG Fusion</h3> <p>How it works</p> <ul> <li>Performs multi query transformation by translating the user’s queries into similar yet distinct through LLM. (same as MultiQueryRetriever)</li> <li>Initialize the vector searches for the original query and its generated similar queries, multiple query generation. (same as MultiQueryRetriever)</li> <li>Combine and refine all the query results using $RRF=\frac{1}{rank+k}$, where $rank$ is the current rank of the documents sorted by distance, and $k$ is a constant smoothing factor that determines the weight given to the existing ranks.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-480.webp 480w,https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-800.webp 800w,https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image Source: <a href="https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1" rel="external nofollow noopener" target="_blank">Adrian H. Raudaschl</a> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># RAG-Fusion: Related
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are a helpful assistant that generates multiple search queries based on a single input query. </span><span class="se">\n</span><span class="s">
Generate multiple search queries related to: {question} </span><span class="se">\n</span><span class="s">
Output (4 queries):</span><span class="sh">"""</span>
<span class="n">prompt_rag_fusion</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.load</span> <span class="kn">import</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span>

<span class="k">def</span> <span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> Reciprocal_rank_fusion that takes multiple lists of ranked documents 
        and an optional parameter k used in the RRF formula </span><span class="sh">"""</span>
    
    <span class="c1"># Initialize a dictionary to hold fused scores for each unique document
</span>    <span class="n">fused_scores</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Iterate through each list of ranked documents
</span>    <span class="k">for</span> <span class="n">docs</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="c1"># Iterate through each document in the list, with its rank (position in the list)
</span>        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
            <span class="c1"># Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)
</span>            <span class="n">doc_str</span> <span class="o">=</span> <span class="nf">dumps</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
            <span class="c1"># If the document is not yet in the fused_scores dictionary, add it with an initial score of 0
</span>            <span class="k">if</span> <span class="n">doc_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">fused_scores</span><span class="p">:</span>
                <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Retrieve the current score of the document, if any
</span>            <span class="n">previous_score</span> <span class="o">=</span> <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span>
            <span class="c1"># Update the score of the document using the RRF formula: 1 / (rank + k)
</span>            <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>

    <span class="c1"># Sort the documents based on their fused scores in descending order to get the final reranked results
</span>    <span class="n">reranked_results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="nf">loads</span><span class="p">(</span><span class="n">doc</span><span class="p">),</span> <span class="n">score</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">fused_scores</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Return the reranked results as a list of tuples, each containing the document and its fused score
</span>    <span class="k">return</span> <span class="n">reranked_results</span>

<span class="n">retrieval_chain_rag_fusion</span> <span class="o">=</span> <span class="n">generate_queries</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">map</span><span class="p">()</span> <span class="o">|</span> <span class="n">reciprocal_rank_fusion</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain_rag_fusion</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
<span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">6</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># RAG
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the following question based on this context:

{context}

Question: {question}
</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">final_rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retrieval_chain_rag_fusion</span><span class="p">,</span> 
     <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nf">itemgetter</span><span class="p">(</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">)}</span> 
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">final_rag_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">involves</span> <span class="n">breaking</span> <span class="n">down</span> <span class="n">large</span> <span class="n">tasks</span> <span class="n">into</span> <span class="n">smaller</span><span class="p">,</span> <span class="n">manageable</span> <span class="n">subgoals</span><span class="p">.</span>
</code></pre></div></div> <h3 id="step-back-prompting">Step Back Prompting</h3> <p>The work of <code class="language-plaintext highlighter-rouge">Step Back Prompting</code> explores how LLMs can tackle complex tasks involving many low-level details through a two-step process of abstraction-and-reasoning.</p> <ul> <li>The first step is to show LLMs how to step back through in-context learning – prompting them to derive high-level abstractions such as concepts and principles for a specific example.</li> <li>The second step is to leverage the reasoning ability to reason on top of the high-level concepts and principles.</li> </ul> <p>The details can be found the paper <a href="https://arxiv.org/pdf/2310.06117" rel="external nofollow noopener" target="_blank">TAKE A STEP BACK</a>.</p> <p>In the following code, we setup a prompting chain to do the abstraction first.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Few Shot Examples to show how to abstract the question
</span><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">FewShotChatMessagePromptTemplate</span>
<span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Could the members of The Police perform lawful arrests?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">what can the members of The Police do?</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># abstraction
</span>    <span class="p">},</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Jan Sindel’s was born in what country?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">what is Jan Sindel’s personal history?</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># abstraction
</span>    <span class="p">},</span>
<span class="p">]</span>
<span class="c1"># We now transform these to example messages
</span><span class="n">example_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{input}</span><span class="sh">"</span><span class="p">),</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">ai</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{output}</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">few_shot_prompt</span> <span class="o">=</span> <span class="nc">FewShotChatMessagePromptTemplate</span><span class="p">(</span>
    <span class="n">example_prompt</span><span class="o">=</span><span class="n">example_prompt</span><span class="p">,</span>
    <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span>
            <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"""</span><span class="s">You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:</span><span class="sh">"""</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="c1"># Few shot examples
</span>        <span class="n">few_shot_prompt</span><span class="p">,</span>
        <span class="c1"># New question
</span>        <span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{question}</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div> <p>Then we can see how it abstract the question “What is task decomposition for LLM agents?”.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">generate_queries_step_back</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span>
<span class="n">generate_queries_step_back</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">What</span> <span class="ow">is</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="ow">in</span> <span class="n">general</span><span class="err">?</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>

<span class="c1"># Response prompt 
</span><span class="n">response_prompt_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.

# {normal_context}
# {step_back_context}

# Original Question: {question}
# Answer:</span><span class="sh">"""</span>
<span class="n">response_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">response_prompt_template</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span>
        <span class="c1"># Retrieve context using the normal question
</span>        <span class="sh">"</span><span class="s">normal_context</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnableLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">])</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">,</span>
        <span class="c1"># Retrieve context using the step-back question
</span>        <span class="sh">"</span><span class="s">step_back_context</span><span class="sh">"</span><span class="p">:</span> <span class="n">generate_queries_step_back</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">,</span>
        <span class="c1"># Pass on the question
</span>        <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="o">|</span> <span class="n">response_prompt</span>
    <span class="o">|</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>

<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">the</span> <span class="n">process</span> <span class="n">of</span> <span class="n">breaking</span> <span class="n">down</span> <span class="nb">complex</span> <span class="n">tasks</span> <span class="n">into</span> <span class="n">smaller</span><span class="p">,</span> <span class="n">more</span> <span class="n">manageable</span> <span class="n">subtasks</span> <span class="n">that</span> <span class="n">can</span> <span class="n">be</span> <span class="n">easily</span> <span class="n">understood</span> <span class="ow">and</span> <span class="n">executed</span> <span class="n">by</span> <span class="n">the</span> <span class="n">large</span> <span class="n">language</span> <span class="nf">model </span><span class="p">(</span><span class="n">LLM</span><span class="p">).</span> <span class="n">This</span> <span class="n">decomposition</span> <span class="n">allows</span> <span class="n">the</span> <span class="n">LLM</span> <span class="n">to</span> <span class="n">effectively</span> <span class="n">parse</span> <span class="n">user</span> <span class="n">requests</span> <span class="ow">and</span> <span class="n">plan</span> <span class="n">out</span> <span class="n">the</span> <span class="n">necessary</span> <span class="n">steps</span> <span class="n">to</span> <span class="n">accomplish</span> <span class="n">the</span> <span class="n">overall</span> <span class="n">task</span><span class="p">.</span>

<span class="o">&gt;</span> <span class="n">In</span> <span class="n">the</span> <span class="n">context</span> <span class="n">of</span> <span class="n">LLM</span><span class="o">-</span><span class="n">powered</span> <span class="n">autonomous</span> <span class="n">agent</span> <span class="n">systems</span><span class="p">,</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">crucial</span> <span class="n">component</span> <span class="n">of</span> <span class="n">the</span> <span class="n">overall</span> <span class="n">workflow</span><span class="bp">...</span>

<span class="o">&gt;</span> <span class="n">Additionally</span><span class="p">,</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="k">with</span> <span class="n">LLM</span> <span class="n">involves</span> <span class="n">simple</span> <span class="n">prompting</span> <span class="n">techniques</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span> <span class="n">providing</span> <span class="n">specific</span> <span class="n">steps</span> <span class="k">for</span> <span class="n">a</span> <span class="nf">task </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span> <span class="sh">"</span><span class="s">Steps for XYZ. 1.</span><span class="sh">"</span><span class="p">)</span> <span class="n">to</span> <span class="n">guide</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">understanding</span> <span class="ow">and</span> <span class="n">executing</span> <span class="n">the</span> <span class="n">task</span> <span class="n">effectively</span><span class="bp">...</span>

<span class="o">&gt;</span> <span class="n">Overall</span><span class="p">,</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">plays</span> <span class="n">a</span> <span class="n">crucial</span> <span class="n">role</span> <span class="ow">in</span> <span class="n">enhancing</span> <span class="n">the</span> <span class="n">performance</span> <span class="ow">and</span> <span class="n">capabilities</span> <span class="n">of</span> <span class="n">autonomous</span> <span class="n">agent</span> <span class="n">systems</span> <span class="n">by</span> <span class="n">enabling</span> <span class="n">effective</span> <span class="n">task</span> <span class="n">planning</span><span class="p">,</span> <span class="n">model</span> <span class="n">selection</span><span class="p">,</span> <span class="ow">and</span> <span class="n">task</span> <span class="n">execution</span><span class="bp">...</span>
</code></pre></div></div> <h3 id="corrective-rag">Corrective RAG</h3> <p><a href="https://arxiv.org/abs/2401.15884" rel="external nofollow noopener" target="_blank">Corrective Retrieval Augmented Generation</a> (CRAG) is proposed to enhance the robustness of generation when errors in retrieval are introduced.</p> <p>Part of CRAG, is a <code class="language-plaintext highlighter-rouge">lightweight trainable retrieval evaluator</code> which assesses the overall quality of retrieved documents, providing a confidence degree to trigger different knowledge retrieval actions.</p> <ul> <li>The retrieval evaluator quantifies a confidence degree, enabling different knowledge retrieval actions such as <code class="language-plaintext highlighter-rouge">Correct</code>, <code class="language-plaintext highlighter-rouge">Incorrect</code>, <code class="language-plaintext highlighter-rouge">Ambiguous</code> based on the assessment.</li> <li>For <code class="language-plaintext highlighter-rouge">Incorrect</code> and <code class="language-plaintext highlighter-rouge">Ambiguous</code> cases, large-scale <code class="language-plaintext highlighter-rouge">web searches</code> are integrated strategically to address limitations in static and limited corpora, aiming to provide a broader and more diverse set of information.</li> </ul> <h3 id="retrieval-augmented-finetuning-raft">Retrieval Augmented FineTuning (RAFT)</h3> <h2 id="evaluation">Evaluation</h2> <p>Langchain provides various types of RAG eval that users of typically interested in:</p> <ul> <li>Response compared against reference answer: metrics like correctness measure “how similar/correct is the answer, relative to a ground-truth label”</li> <li>Response compared against retrieved docs: metrics like faithfulness, hallucinations, etc. measure “to what extent does the generated response agree with the retrieved context”</li> <li>Retrieved docs compared against input: metrics like score @ k, mean reciprocal rank, NDCG, etc. measure “how good are my retrieved results for this query”</li> </ul> <table> <thead> <tr> <th style="text-align: left">Target</th> <th style="text-align: center">Explain</th> <th style="text-align: right">Metrics</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Answer correctness</td> <td style="text-align: center">how similar/correct is the answer, relative to a ground-truth label</td> <td style="text-align: right">accuracy-ratio</td> </tr> <tr> <td style="text-align: left">Answer faithfulness</td> <td style="text-align: center">to what extent does the generated response agree with the retrieved context</td> <td style="text-align: right">faithfulness/hallucination ratio</td> </tr> <tr> <td style="text-align: left">Retro relevance</td> <td style="text-align: center">how good are the retrieved results for this query</td> <td style="text-align: right">score@k, mean reciprocal rank</td> </tr> </tbody> </table> <p>The full code snippet can be found at <a href="https://docs.smith.langchain.com/cookbook/testing-examples/rag_eval" rel="external nofollow noopener" target="_blank">LangSmith-RAG-Eval</a>.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://github.com/langchain-ai/rag-from-scratch/tree/main" rel="external nofollow noopener" target="_blank">LangChain - rag from scrach</a></li> <li><a href="https://github.com/Raudaschl/rag-fusion/blob/master/main.py" rel="external nofollow noopener" target="_blank">RAG-Fusion</a></li> <li><a href="https://arxiv.org/abs/2401.15884" rel="external nofollow noopener" target="_blank">CRAG</a></li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Siqiao Xue. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>