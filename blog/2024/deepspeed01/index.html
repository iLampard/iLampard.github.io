<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Training with DeepSpeed - Basic Concepts | Siqiao Xue </title> <meta name="author" content="Siqiao Xue"> <meta name="description" content="basic concepts behind of DeepSpeed"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ilampard.github.io/blog/2024/deepspeed01/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Training with DeepSpeed - Basic Concepts",
            "description": "basic concepts behind of DeepSpeed",
            "published": "April 30, 2024",
            "authors": [
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Siqiao</span> Xue </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/archive/">archive </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Training with DeepSpeed - Basic Concepts</h1> <p>basic concepts behind of DeepSpeed</p> </d-title> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#concepts-of-model-parallelism">Concepts of Model Parallelism</a> </div> <ul> <li> <a href="#dataparallel-dp">DataParallel (DP)</a> </li> <li> <a href="#tensorparallel-tp">TensorParallel (TP)</a> </li> <li> <a href="#naive-model-parallel-nmp-and-pipelineparallel-pp">Naive Model Parallel (NMP) and PipelineParallel (PP)</a> </li> </ul> <div> <a href="#memory-consumption-in-model-training">Memory Consumption in Model Training</a> </div> <div> <a href="#reference">Reference</a> </div> </nav> </d-contents> <h2 id="concepts-of-model-parallelism">Concepts of Model Parallelism</h2> <p>There are a few concepts that need to be reviewed: DataParallel (DP), TensorParallel (TP), PipelineParallel (PP).</p> <h3 id="dataparallel-dp">DataParallel (DP)</h3> <p>In DP, the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step.</p> <p>Consider a simple model with 3 layers, where each layer has 2 params:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>La | Lb | Lc
<span class="nt">---</span>|----|---
a0 | b0 | c0
a1 | b1 | c1
</code></pre></div></div> <p>If we have two GPUs, DP splits the model onto 2 GPUs like so:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU0: 
L0 | L1 | L2
<span class="nt">---</span>|----|---
a0 | b0 | c0
a1 | b1 | c1

GPU1:
L0 | L1 | L2
<span class="nt">---</span>|----|---
a0 | b0 | c0
a1 | b1 | c1
</code></pre></div></div> <p>Each GPU holds a model, and with every iteration (step), the batch data is divided into 2 equally sized micro-batches. Each GPU independently calculates the gradients based on the micro-batch data it receives. Then the gradient is averaged by the Allreduce algorithm across all GPUs.</p> <h3 id="tensorparallel-tp">TensorParallel (TP)</h3> <p>Each tensor is split up into multiple chunks. Each shard of the tensor resides on its designated gpu. During processing each shard gets processed separately and in parallel on different GPUs and the results are synced at the end of the step. This is also called horizontal parallelism, as the splitting happens on horizontal level.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU0:
L0 | L1 | L2
<span class="nt">---</span>|----|---
a0 | b0 | c0

GPU1:
L0 | L1 | L2
<span class="nt">---</span>|----|---
a1 | b1 | c1
</code></pre></div></div> <h3 id="naive-model-parallel-nmp-and-pipelineparallel-pp">Naive Model Parallel (NMP) and PipelineParallel (PP)</h3> <p>In <code class="language-plaintext highlighter-rouge">Naive Model Parallel (MP)</code>, the model is split up vertically (layer-level) across multiple GPUs, so that only one or several layers of the model are places on a single gpu. Each gpu processes in parallel different stages of the pipeline and working on a small chunk of the batch.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">================</span>   <span class="o">=================</span>
|  L0 | L1 | L2 |   | L3 | L4 | L5 |
<span class="o">=================</span>  <span class="o">=================</span>
        GPU0                 GPU1
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">Pipeline Parallel (PP)</code> is almost identical to a naive MP, but it solves the GPU <code class="language-plaintext highlighter-rouge">idling problem</code>, by chunking the incoming batch into micro-batches and artificially creating a pipeline, which allows different GPUs to concurrently participate in the computation process.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble-480.webp 480w,https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble-800.webp 800w,https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image Source: <a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism" rel="external nofollow noopener" target="_blank">HuggingFace Documentation</a> </div> <h2 id="memory-consumption-in-model-training">Memory Consumption in Model Training</h2> <p>During model training, most of the memory is consumed by two things:</p> <ul> <li> <code class="language-plaintext highlighter-rouge">Model states</code>, which includes tensors comprising of optimizer states, gradients, and parameters.</li> <li> <code class="language-plaintext highlighter-rouge">Activations</code>, which includes any tensor that is created in the forward pass and is necessary for gradient computation during backward pass.</li> </ul> <p>The state-of-the-art approach to train LLMs on the current generation of NVIDIA GPUs is via mixed precision.</p> <p>Assume we train a model with $\Uppsi$ parameters using Adam. This requires to $2\Uppsi + 2\Uppsi + 3*4\Uppsi = 16\Uppsi$ bytes of memory requirement (see our blog <a href="https://ilampard.github.io/blog/2024/mixed-precision/" rel="external nofollow noopener" target="_blank">Mixed-precision training in LLM</a> for details).</p> <h2 id="reference">Reference</h2> <p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism" rel="external nofollow noopener" target="_blank">Model Parallelism</a> <a href="https://blog.eleuther.ai/transformer-math/" rel="external nofollow noopener" target="_blank">Transformer Math</a> <a href="https://zhuanlan.zhihu.com/p/613196255" rel="external nofollow noopener" target="_blank">图解大模型训练之：流水线并行（Pipeline Parallelism），以Gpipe为例</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Siqiao Xue. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>