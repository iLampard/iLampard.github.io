<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://ilampard.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ilampard.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-04-30T13:09:27+00:00</updated><id>https://ilampard.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html"></title><link href="https://ilampard.github.io/blog/2024/2024-04-30-deepspeed01/" rel="alternate" type="text/html" title=""/><published>2024-04-30T13:09:27+00:00</published><updated>2024-04-30T13:09:27+00:00</updated><id>https://ilampard.github.io/blog/2024/2024-04-30-deepspeed01</id><content type="html" xml:base="https://ilampard.github.io/blog/2024/2024-04-30-deepspeed01/"><![CDATA[<h2 id="concepts-of-model-parallelism">Concepts of Model Parallelism</h2> <p>There are a few concepts that need to be reviewed: DataParallel (DP),</p> <h3 id="dataparallel-dp">DataParallel (DP)</h3> <p>In DP, the same setup is replicated multiple times, and each being fed a slice of the data. The processing is done in parallel and all setups are synchronized at the end of each training step.</p> <p>Consider a simple model with 3 layers, where each layer has 2 params:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>La | Lb | Lc
<span class="nt">---</span>|----|---
a0 | b0 | c0
a1 | b1 | c1
</code></pre></div></div> <p>If we have two GPUs, DP splits the model onto 2 GPUs like so:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>GPU0: 
L0 | L1 | L2
<span class="nt">---</span>|----|---
a0 | b0 | c0
a1 | b1 | c1

GPU1:
L0 | L1 | L2
<span class="nt">---</span>|----|---
a0 | b0 | c0
a1 | b1 | c1
</code></pre></div></div> <p>Each GPU holds a model, and with every iteration (step), the batch data is divided into 2 equally sized micro-batches. Each GPU independently calculates the gradients based on the micro-batch data it receives. Then the gradient is averaged by the Allreduce algorithm across all GPUs.</p> <h2 id="reference">Reference</h2> <p><a href="https://huggingface.co/docs/transformers/v4.15.0/parallelism">Model Parallelism</a></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;!--
  See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
  https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
--&gt;

  &lt;source
    class="responsive-img-srcset"
    srcset="/assets/img/9-480.webp 480w,/assets/img/9-800.webp 800w,/assets/img/9-1400.webp 1400w,"
    
      sizes="95vw"
    
    type="image/webp"
  &gt;

&lt;img
  src="/assets/img/9.jpg"
  
    class="img-fluid rounded z-depth-1"
  
  
    width="100%"
  
  
    height="auto"
  
  
  
  
  
  
    loading="eager"
  
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt; &lt;/swiper-slide&gt;</p> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7-480.webp 480w,/assets/img/7-800.webp 800w,/assets/img/7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/7.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/8-480.webp 480w,/assets/img/8-800.webp 800w,/assets/img/8-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/8.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/10-480.webp 480w,/assets/img/10-800.webp 800w,/assets/img/10-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/10.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <swiper-slide> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/12-480.webp 480w,/assets/img/12-800.webp 800w,/assets/img/12-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/12.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </swiper-slide> <p>&lt;/swiper-container&gt;</p> <h2 id="image-comparison-slider">Image Comparison Slider</h2> <p>This is a simple image comparison slider. It uses the <a href="https://img-comparison-slider.sneas.io/">img-comparison-slider</a> library. Check the <a href="https://img-comparison-slider.sneas.io/examples.html">examples page</a> for more information of what you can achieve with it.</p> <img-comparison-slider> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic_color-480.webp 480w,/assets/img/prof_pic_color-800.webp 800w,/assets/img/prof_pic_color-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/prof_pic_color.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </img-comparison-slider> <p>–&gt;</p>]]></content><author><name></name></author></entry><entry><title type="html">Basic and Advanced Techniques on RAG</title><link href="https://ilampard.github.io/blog/2024/techniques-rag/" rel="alternate" type="text/html" title="Basic and Advanced Techniques on RAG"/><published>2024-04-28T00:20:00+00:00</published><updated>2024-04-28T00:20:00+00:00</updated><id>https://ilampard.github.io/blog/2024/techniques-rag</id><content type="html" xml:base="https://ilampard.github.io/blog/2024/techniques-rag/"><![CDATA[<h2 id="introduction-to-rag">Introduction to RAG</h2> <p>Many LLM applications require user-specific data that is not part of the model’s training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In RAG process, external data is retrieved and then passed to the LLM when doing the generation step. We take <a href="https://python.langchain.com/docs/get_started/introduction">Langchain</a> as the codebase to understand this process.</p> <p>RAG typically involves three process: indexing, retrieval and generation.</p> <h3 id="indexing">Indexing</h3> <p>In indexing process, the systems sync documents from external source into a vector store. Assume we load a website as a document.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain_community</span> <span class="n">tiktoken</span> <span class="n">langchain</span><span class="o">-</span><span class="n">openai</span> <span class="n">langchainhub</span> <span class="n">chromadb</span> <span class="n">langchain</span> <span class="n">langchain_text_splitters</span> <span class="n">sentence_transformers</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load blog
</span><span class="kn">import</span> <span class="n">bs4</span>
<span class="kn">from</span> <span class="n">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">WebBaseLoader</span>
<span class="n">loader</span> <span class="o">=</span> <span class="nc">WebBaseLoader</span><span class="p">(</span>
    <span class="n">web_paths</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">"</span><span class="p">,),</span>
    <span class="n">bs_kwargs</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span>
        <span class="n">parse_only</span><span class="o">=</span><span class="n">bs4</span><span class="p">.</span><span class="nc">SoupStrainer</span><span class="p">(</span>
            <span class="n">class_</span><span class="o">=</span><span class="p">(</span><span class="sh">"</span><span class="s">post-content</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">post-title</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">post-header</span><span class="sh">"</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="n">blog_docs</span> <span class="o">=</span> <span class="n">loader</span><span class="p">.</span><span class="nf">load</span><span class="p">()</span>

<span class="nf">len</span><span class="p">(</span><span class="n">blog_docs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">page_content</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">43131</span>
</code></pre></div></div> <p>Once loaded, we need to split the long document into smaller chunks that can fit into the model’s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.</p> <p>Base on LangChain’s document, at a high level, text splitters firstly split the text up into small, semantically meaningful chunks (often sentences); then combine these small chunks into a larger chunk until reaching a certain size (as measured by some function). Once reaching that size, the splitter makes that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).</p> <p>By default, it is recommended to use RecursiveCharacterTextSplitter for generic text. It is parameterized by a list of characters (The default list is <code class="language-plaintext highlighter-rouge">["\n\n", "\n", " ", ""]</code>.). It tries to split on them in order until the chunks are small enough. It has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>

<span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">RecursiveCharacterTextSplitter</span><span class="p">(</span>
    <span class="c1"># Set a really small chunk size, just to show.
</span>    <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="n">length_function</span><span class="o">=</span><span class="nb">len</span><span class="p">,</span>
    <span class="n">is_separator_regex</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Make splits
</span><span class="n">splits</span> <span class="o">=</span> <span class="n">text_splitter</span><span class="p">.</span><span class="nf">split_documents</span><span class="p">(</span><span class="n">blog_docs</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">LLM Powered Autonomous Agents</span><span class="sh">'</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="n">splits</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng</span><span class="sh">'</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">splits</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="mi">611</span>
</code></pre></div></div> <h3 id="embedding-model">Embedding Model</h3> <p>The embedding model creates a vector representation for a piece of text so we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.embeddings</span> <span class="kn">import</span> <span class="n">HuggingFaceBgeEmbeddings</span>

<span class="n">model_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">BAAI/bge-small-en</span><span class="sh">"</span>
<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">device</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">}</span>
<span class="n">encode_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">normalize_embeddings</span><span class="sh">"</span><span class="p">:</span> <span class="bp">True</span><span class="p">}</span>
<span class="n">hf</span> <span class="o">=</span> <span class="nc">HuggingFaceBgeEmbeddings</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span> <span class="n">encode_kwargs</span><span class="o">=</span><span class="n">encode_kwargs</span>
<span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">emb</span> <span class="o">=</span> <span class="n">hf</span><span class="p">.</span><span class="nf">embed_query</span><span class="p">(</span><span class="sh">"</span><span class="s">hi this is harrison</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">emb</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="mi">384</span>
</code></pre></div></div> <p>We can initialize the vector store object following the following example</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>

<span class="n">vectorstore</span> <span class="o">=</span> <span class="n">Chroma</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span> 
                                    <span class="n">embedding</span><span class="o">=</span><span class="n">hf</span><span class="p">)</span>
</code></pre></div></div> <h3 id="retriever">Retriever</h3> <p>A retriever is an interface that returns documents given an unstructured query.</p> <p>Retriever vs vectorstore:</p> <ul> <li>A retriever is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them.</li> <li>Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">retriever</span> <span class="o">=</span> <span class="n">vectorstore</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">()</span>

<span class="n">docs</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">get_relevant_documents</span><span class="p">(</span><span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="nc">Document</span><span class="p">(</span><span class="n">page_content</span><span class="o">=</span><span class="sh">'</span><span class="s">Task decomposition can be done (1) by LLM with simple prompting like </span><span class="sh">"</span><span class="s">Steps for XYZ.</span><span class="se">\\</span><span class="s">n1.</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="s">What</span><span class="sh">'</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">https://lilianweng.github.io/posts/2023-06-23-agent/</span><span class="sh">'</span><span class="p">})]</span>
</code></pre></div></div> <h2 id="advanced-techniques-in-retrievers">Advanced Techniques in Retrievers</h2> <h3 id="query-rewriting">Query Rewriting</h3> <p>The input query can be ambiguous, causing an inevitab gap between the input text and the knowledge that is really needed to query.</p> <p>A straightforward way is to use LLM to generate queries from multiple perspective, a.k.a. multi-query transform.</p> <p>The whole process: for a given user input query, it uses an LLM to generate multiple queries from different perspectives. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. Lastly, it feeds all the retrieved documents and let LLM to generate the answer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># Multi Query - generate queries from different perspectives using a prompt
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are an AI language model assistant. Your task is to generate five
different versions of the given user question to retrieve relevant documents from a vector
database. By generating multiple perspectives on the user question, your goal is to help
the user overcome some of the limitations of the distance-based similarity search.
Provide these alternative questions separated by newlines. Original question: {question}</span><span class="sh">"""</span>
<span class="n">prompt_perspectives</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="kn">from</span> <span class="n">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>

<span class="kn">import</span> <span class="n">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">'</span><span class="s">OPENAI_API_KEY</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">xxx</span><span class="sh">'</span>

<span class="n">generate_queries</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">prompt_perspectives</span>
    <span class="o">|</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
    <span class="o">|</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">))</span>
<span class="p">)</span>
</code></pre></div></div> <p>By applying the multi-query approach, we retrieve more documents than the standard approach and run the retrieval process to get an anaswer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.load</span> <span class="kn">import</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span>

<span class="k">def</span> <span class="nf">get_unique_union</span><span class="p">(</span><span class="n">documents</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">]):</span>
    <span class="sh">"""</span><span class="s"> Unique union of retrieved docs </span><span class="sh">"""</span>
    <span class="c1"># Flatten list of lists, and convert each Document to string
</span>    <span class="n">flattened_docs</span> <span class="o">=</span> <span class="p">[</span><span class="nf">dumps</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">documents</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
    <span class="c1"># Get unique documents
</span>    <span class="n">unique_docs</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">flattened_docs</span><span class="p">))</span>
    <span class="c1"># Return
</span>    <span class="k">return</span> <span class="p">[</span><span class="nf">loads</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">unique_docs</span><span class="p">]</span>

<span class="c1"># Retrieve process
</span><span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span>

<span class="c1"># pass each query to the retriever and remove the duplicate docs
</span><span class="n">retrieval_chain</span> <span class="o">=</span> <span class="n">generate_queries</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">map</span><span class="p">()</span> <span class="o">|</span> <span class="n">get_unique_union</span>

<span class="c1"># retrieve
</span><span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">6</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">operator</span> <span class="kn">import</span> <span class="n">itemgetter</span>
<span class="kn">from</span> <span class="n">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># RAG
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the following question based on this context:

{context}

Question: {question}
</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">llm</span> <span class="o">=</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">final_rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retrieval_chain</span><span class="p">,</span> 
     <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nf">itemgetter</span><span class="p">(</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">)}</span> 
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">final_rag_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">involves</span> <span class="n">parsing</span> <span class="n">user</span> <span class="n">requests</span> <span class="n">into</span> <span class="n">multiple</span> <span class="n">tasks</span><span class="p">,</span> <span class="k">with</span> <span class="n">the</span> <span class="n">LLM</span> <span class="n">acting</span> <span class="k">as</span> <span class="n">the</span> <span class="n">brain</span> <span class="n">to</span> <span class="n">organize</span> <span class="ow">and</span> <span class="n">manage</span> <span class="n">these</span> <span class="n">tasks</span><span class="p">.</span>
</code></pre></div></div> <h3 id="rag-fusion">RAG Fusion</h3> <p>How it works</p> <ul> <li>Performs multi query transformation by translating the user’s queries into similar yet distinct through LLM. (same as MultiQueryRetriever)</li> <li>Initialize the vector searches for the original query and its generated similar queries, multiple query generation. (same as MultiQueryRetriever)</li> <li>Combine and refine all the query results using $RRF=\frac{1}{rank+k}$, where $rank$ is the current rank of the documents sorted by distance, and $k$ is a constant smoothing factor that determines the weight given to the existing ranks.</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-480.webp 480w,https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-800.webp 800w,https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*acPUjXj6kIeJHxV5Fgjf9g.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Image Source: <a href="https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1">Adrian H. Raudaschl</a> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>

<span class="c1"># RAG-Fusion: Related
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are a helpful assistant that generates multiple search queries based on a single input query. </span><span class="se">\n</span><span class="s">
Generate multiple search queries related to: {question} </span><span class="se">\n</span><span class="s">
Output (4 queries):</span><span class="sh">"""</span>
<span class="n">prompt_rag_fusion</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain.load</span> <span class="kn">import</span> <span class="n">dumps</span><span class="p">,</span> <span class="n">loads</span>

<span class="k">def</span> <span class="nf">reciprocal_rank_fusion</span><span class="p">(</span><span class="n">results</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">60</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s"> Reciprocal_rank_fusion that takes multiple lists of ranked documents 
        and an optional parameter k used in the RRF formula </span><span class="sh">"""</span>
    
    <span class="c1"># Initialize a dictionary to hold fused scores for each unique document
</span>    <span class="n">fused_scores</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="c1"># Iterate through each list of ranked documents
</span>    <span class="k">for</span> <span class="n">docs</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="c1"># Iterate through each document in the list, with its rank (position in the list)
</span>        <span class="k">for</span> <span class="n">rank</span><span class="p">,</span> <span class="n">doc</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">docs</span><span class="p">):</span>
            <span class="c1"># Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)
</span>            <span class="n">doc_str</span> <span class="o">=</span> <span class="nf">dumps</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span>
            <span class="c1"># If the document is not yet in the fused_scores dictionary, add it with an initial score of 0
</span>            <span class="k">if</span> <span class="n">doc_str</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">fused_scores</span><span class="p">:</span>
                <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># Retrieve the current score of the document, if any
</span>            <span class="n">previous_score</span> <span class="o">=</span> <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span>
            <span class="c1"># Update the score of the document using the RRF formula: 1 / (rank + k)
</span>            <span class="n">fused_scores</span><span class="p">[</span><span class="n">doc_str</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="n">k</span><span class="p">)</span>

    <span class="c1"># Sort the documents based on their fused scores in descending order to get the final reranked results
</span>    <span class="n">reranked_results</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="nf">loads</span><span class="p">(</span><span class="n">doc</span><span class="p">),</span> <span class="n">score</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">doc</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nf">sorted</span><span class="p">(</span><span class="n">fused_scores</span><span class="p">.</span><span class="nf">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="c1"># Return the reranked results as a list of tuples, each containing the document and its fused score
</span>    <span class="k">return</span> <span class="n">reranked_results</span>

<span class="n">retrieval_chain_rag_fusion</span> <span class="o">=</span> <span class="n">generate_queries</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">map</span><span class="p">()</span> <span class="o">|</span> <span class="n">reciprocal_rank_fusion</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">retrieval_chain_rag_fusion</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
<span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">6</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnablePassthrough</span>

<span class="c1"># RAG
</span><span class="n">template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">Answer the following question based on this context:

{context}

Question: {question}
</span><span class="sh">"""</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">template</span><span class="p">)</span>

<span class="n">final_rag_chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">retrieval_chain_rag_fusion</span><span class="p">,</span> 
     <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="nf">itemgetter</span><span class="p">(</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">)}</span> 
    <span class="o">|</span> <span class="n">prompt</span>
    <span class="o">|</span> <span class="n">llm</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">final_rag_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span><span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">involves</span> <span class="n">breaking</span> <span class="n">down</span> <span class="n">large</span> <span class="n">tasks</span> <span class="n">into</span> <span class="n">smaller</span><span class="p">,</span> <span class="n">manageable</span> <span class="n">subgoals</span><span class="p">.</span>
</code></pre></div></div> <h3 id="step-back-prompting">Step Back Prompting</h3> <p>The work of <code class="language-plaintext highlighter-rouge">Step Back Prompting</code> explores how LLMs can tackle complex tasks involving many low-level details through a two-step process of abstraction-and-reasoning.</p> <ul> <li>The first step is to show LLMs how to step back through in-context learning – prompting them to derive high-level abstractions such as concepts and principles for a specific example.</li> <li>The second step is to leverage the reasoning ability to reason on top of the high-level concepts and principles.</li> </ul> <p>The details can be found the paper <a href="https://arxiv.org/pdf/2310.06117">TAKE A STEP BACK</a>.</p> <p>In the following code, we setup a prompting chain to do the abstraction first.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Few Shot Examples to show how to abstract the question
</span><span class="kn">from</span> <span class="n">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span><span class="p">,</span> <span class="n">FewShotChatMessagePromptTemplate</span>
<span class="n">examples</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Could the members of The Police perform lawful arrests?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">what can the members of The Police do?</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># abstraction
</span>    <span class="p">},</span>
    <span class="p">{</span>
        <span class="sh">"</span><span class="s">input</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">Jan Sindel’s was born in what country?</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">output</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">what is Jan Sindel’s personal history?</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># abstraction
</span>    <span class="p">},</span>
<span class="p">]</span>
<span class="c1"># We now transform these to example messages
</span><span class="n">example_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">human</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{input}</span><span class="sh">"</span><span class="p">),</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">ai</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{output}</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">few_shot_prompt</span> <span class="o">=</span> <span class="nc">FewShotChatMessagePromptTemplate</span><span class="p">(</span>
    <span class="n">example_prompt</span><span class="o">=</span><span class="n">example_prompt</span><span class="p">,</span>
    <span class="n">examples</span><span class="o">=</span><span class="n">examples</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span>
            <span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"""</span><span class="s">You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:</span><span class="sh">"""</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="c1"># Few shot examples
</span>        <span class="n">few_shot_prompt</span><span class="p">,</span>
        <span class="c1"># New question
</span>        <span class="p">(</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">{question}</span><span class="sh">"</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div> <p>Then we can see how it abstract the question “What is task decomposition for LLM agents?”.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">generate_queries_step_back</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="n">question</span> <span class="o">=</span> <span class="sh">"</span><span class="s">What is task decomposition for LLM agents?</span><span class="sh">"</span>
<span class="n">generate_queries_step_back</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>
<span class="o">&gt;</span> <span class="n">What</span> <span class="ow">is</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="ow">in</span> <span class="n">general</span><span class="err">?</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">from</span> <span class="n">langchain_core.runnables</span> <span class="kn">import</span> <span class="n">RunnableLambda</span>

<span class="c1"># Response prompt 
</span><span class="n">response_prompt_template</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.

# {normal_context}
# {step_back_context}

# Original Question: {question}
# Answer:</span><span class="sh">"""</span>
<span class="n">response_prompt</span> <span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="p">.</span><span class="nf">from_template</span><span class="p">(</span><span class="n">response_prompt_template</span><span class="p">)</span>

<span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span>
        <span class="c1"># Retrieve context using the normal question
</span>        <span class="sh">"</span><span class="s">normal_context</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnableLambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">])</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">,</span>
        <span class="c1"># Retrieve context using the step-back question
</span>        <span class="sh">"</span><span class="s">step_back_context</span><span class="sh">"</span><span class="p">:</span> <span class="n">generate_queries_step_back</span> <span class="o">|</span> <span class="n">retriever</span><span class="p">,</span>
        <span class="c1"># Pass on the question
</span>        <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">],</span>
    <span class="p">}</span>
    <span class="o">|</span> <span class="n">response_prompt</span>
    <span class="o">|</span> <span class="nc">ChatOpenAI</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="o">|</span> <span class="nc">StrOutputParser</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">question</span><span class="p">})</span>

<span class="o">&gt;</span> <span class="n">Task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">refers</span> <span class="n">to</span> <span class="n">the</span> <span class="n">process</span> <span class="n">of</span> <span class="n">breaking</span> <span class="n">down</span> <span class="nb">complex</span> <span class="n">tasks</span> <span class="n">into</span> <span class="n">smaller</span><span class="p">,</span> <span class="n">more</span> <span class="n">manageable</span> <span class="n">subtasks</span> <span class="n">that</span> <span class="n">can</span> <span class="n">be</span> <span class="n">easily</span> <span class="n">understood</span> <span class="ow">and</span> <span class="n">executed</span> <span class="n">by</span> <span class="n">the</span> <span class="n">large</span> <span class="n">language</span> <span class="nf">model </span><span class="p">(</span><span class="n">LLM</span><span class="p">).</span> <span class="n">This</span> <span class="n">decomposition</span> <span class="n">allows</span> <span class="n">the</span> <span class="n">LLM</span> <span class="n">to</span> <span class="n">effectively</span> <span class="n">parse</span> <span class="n">user</span> <span class="n">requests</span> <span class="ow">and</span> <span class="n">plan</span> <span class="n">out</span> <span class="n">the</span> <span class="n">necessary</span> <span class="n">steps</span> <span class="n">to</span> <span class="n">accomplish</span> <span class="n">the</span> <span class="n">overall</span> <span class="n">task</span><span class="p">.</span>

<span class="o">&gt;</span> <span class="n">In</span> <span class="n">the</span> <span class="n">context</span> <span class="n">of</span> <span class="n">LLM</span><span class="o">-</span><span class="n">powered</span> <span class="n">autonomous</span> <span class="n">agent</span> <span class="n">systems</span><span class="p">,</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">crucial</span> <span class="n">component</span> <span class="n">of</span> <span class="n">the</span> <span class="n">overall</span> <span class="n">workflow</span><span class="bp">...</span>

<span class="o">&gt;</span> <span class="n">Additionally</span><span class="p">,</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="k">with</span> <span class="n">LLM</span> <span class="n">involves</span> <span class="n">simple</span> <span class="n">prompting</span> <span class="n">techniques</span><span class="p">,</span> <span class="n">such</span> <span class="k">as</span> <span class="n">providing</span> <span class="n">specific</span> <span class="n">steps</span> <span class="k">for</span> <span class="n">a</span> <span class="nf">task </span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">g</span><span class="p">.,</span> <span class="sh">"</span><span class="s">Steps for XYZ. 1.</span><span class="sh">"</span><span class="p">)</span> <span class="n">to</span> <span class="n">guide</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">understanding</span> <span class="ow">and</span> <span class="n">executing</span> <span class="n">the</span> <span class="n">task</span> <span class="n">effectively</span><span class="bp">...</span>

<span class="o">&gt;</span> <span class="n">Overall</span><span class="p">,</span> <span class="n">task</span> <span class="n">decomposition</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">agents</span> <span class="n">plays</span> <span class="n">a</span> <span class="n">crucial</span> <span class="n">role</span> <span class="ow">in</span> <span class="n">enhancing</span> <span class="n">the</span> <span class="n">performance</span> <span class="ow">and</span> <span class="n">capabilities</span> <span class="n">of</span> <span class="n">autonomous</span> <span class="n">agent</span> <span class="n">systems</span> <span class="n">by</span> <span class="n">enabling</span> <span class="n">effective</span> <span class="n">task</span> <span class="n">planning</span><span class="p">,</span> <span class="n">model</span> <span class="n">selection</span><span class="p">,</span> <span class="ow">and</span> <span class="n">task</span> <span class="n">execution</span><span class="bp">...</span>
</code></pre></div></div> <h2 id="reference">Reference</h2> <ul> <li><a href="https://github.com/langchain-ai/rag-from-scratch/tree/main">LangChain - rag from scrach</a></li> <li><a href="https://github.com/Raudaschl/rag-fusion/blob/master/main.py">RAG-Fusion</a></li> </ul> ]]></content><author><name></name></author><category term="LLM"/><category term="RAG"/><category term="code"/><summary type="html"><![CDATA[detailed techniques on RAG, with LangChain code examples]]></summary></entry><entry><title type="html">Mixed-precision training in LLM</title><link href="https://ilampard.github.io/blog/2024/mixed-precision/" rel="alternate" type="text/html" title="Mixed-precision training in LLM"/><published>2024-04-23T19:22:00+00:00</published><updated>2024-04-23T19:22:00+00:00</updated><id>https://ilampard.github.io/blog/2024/mixed-precision</id><content type="html" xml:base="https://ilampard.github.io/blog/2024/mixed-precision/"><![CDATA[<h2 id="background">Background</h2> <h3 id="float-precision-in-deep-learning">Float Precision in Deep Learning</h3> <p>In the realm of deep learning, using 64-bit floating point operations is considered unnecessary and computationally expensive since 64-bit operations are generally more costly, and GPU hardware is also not optimized for 64-bit precision. So instead, 32-bit floating point operations (also known as single-precision) have become the standard for training deep neural networks on GPUs. In fact, PyTorch uses 32-bit floats by default.</p> <h3 id="technical-background-on-floating-point-representation">Technical Background on Floating-point Representation</h3> <p>In the context of floating-point numbers, “bits” refer to the binary digits used to represent a number in a computer’s memory. In floating-point representation, numbers are stored in a combination of three parts: the sign, the exponent (the power number of 2), and the significand (faction value).</p> <p>There are three popular floating point formats</p> <ul> <li>Float32: sign 1 bit, exponent 8 bit and fraction 23 bit.</li> <li>Float16: sign 1 bit, exponent 5 bit and fraction 10 bit.</li> <li>BFloat16 (Brain Floating Point): sign 1, exponent 8 and fraction 7 bit.</li> </ul> <h4 id="float32-vs-float16">Float32 vs Float16</h4> <p>Float16 uses three fewer bits for the exponent and 13 fewer bits for the fractional value: it represent a narrower range of numbers with less precisions.</p> <h4 id="float32-vs-float16-vs-bfloat16">Float32 vs Float16 vs BFloat16</h4> <p>Float32 and BFloat16 represent the same range of values as their exponents both have 8 bits. Compared to Float32 and Float16, BFloat16 has lowest precision. But in most applications, this reduced precision has minimal impact on modeling performance.</p> <p>The code below reveals that the largest float32 number is 3.40282e+38; float16 numbers cannot exceed the value 65,504.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">finfo</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float16</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="nf">finfo</span><span class="p">(</span><span class="n">resolution</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mi">65504</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">65504</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.000976562</span><span class="p">,</span> <span class="n">smallest_normal</span><span class="o">=</span><span class="mf">6.10352e-05</span><span class="p">,</span> <span class="n">tiny</span><span class="o">=</span><span class="mf">6.10352e-05</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float16</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">finfo</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="nf">finfo</span><span class="p">(</span><span class="n">resolution</span><span class="o">=</span><span class="mf">1e-06</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">3.40282e+38</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">3.40282e+38</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1.19209e-07</span><span class="p">,</span> <span class="n">smallest_normal</span><span class="o">=</span><span class="mf">1.17549e-38</span><span class="p">,</span> <span class="n">tiny</span><span class="o">=</span><span class="mf">1.17549e-38</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># torch.cuda.is_bf16_supported() # check if bfloat16 is suppored in cuda
</span><span class="n">torch</span><span class="p">.</span><span class="nf">finfo</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="nf">finfo</span><span class="p">(</span><span class="n">resolution</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="mf">3.38953e+38</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">3.38953e+38</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.0078125</span><span class="p">,</span> <span class="n">smallest_normal</span><span class="o">=</span><span class="mf">1.17549e-38</span><span class="p">,</span> <span class="n">tiny</span><span class="o">=</span><span class="mf">1.17549e-38</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">bfloat16</span><span class="p">)</span>

</code></pre></div></div> <h2 id="mixed-precision-training">Mixed-Precision Training</h2> <p>Instead of running all parameters and operations on Float16, we switch between 32-bit and 16-bit operations during training, hence, the term “mixed” precision.</p> <ul> <li>step 1: convert 32-bit weights to 16-bit weights of neworks, for faster computation.</li> <li>step 2: compute gradient using 16-bit precision.</li> <li>step 3: convert 16-bit gradint to 32-bit gradient to maintain numerical stability.</li> <li>step 4: multiplied by learning rate and update weight in 32-bit precision.</li> </ul> <p>Therefore, we have one copy of model weight and gradient in 16-bit; one copy of gradient and optimizer in 32-bit.</p> <h2 id="reference">Reference</h2> <ul> <li><a href="https://sebastianraschka.com/blog/2023/llm-mixed-precision-copy.html">Accelerating Large Language Models with Mixed-Precision Techniques</a></li> <li><a href="https://github.com/Glanvery/LLM-Travel">Github - LLM-Travel</a></li> </ul>]]></content><author><name></name></author><category term="LLM"/><category term="pre-training"/><category term="code"/><summary type="html"><![CDATA[a note on mixed-precision training]]></summary></entry><entry><title type="html">Understanding tokenizer from Andrej Karpathy’s tutorial</title><link href="https://ilampard.github.io/blog/2024/tokenizer/" rel="alternate" type="text/html" title="Understanding tokenizer from Andrej Karpathy’s tutorial"/><published>2024-04-14T19:22:00+00:00</published><updated>2024-04-14T19:22:00+00:00</updated><id>https://ilampard.github.io/blog/2024/tokenizer</id><content type="html" xml:base="https://ilampard.github.io/blog/2024/tokenizer/"><![CDATA[<h2 id="background">Background</h2> <h3 id="what-is-tokenizer">What is Tokenizer</h3> <p>A tokenizer is in charge of preparing the inputs for a model. It is used to split the text into tokens available in the predefined vocabulary and convert tokens strings to ids and back.</p> <p>Shown below, we split a sentence using the GPT-2 tokenizer. “I have an egg” has been split into five tokens, along with the space in between the words and ‘!’ punctuation. A visualization playground can be found at <a href="https://tiktokenizer.vercel.app/?encoder=gpt2">vercel</a>.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">openai-community/gpt2</span><span class="sh">"</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="p">.</span><span class="nf">tokenize</span><span class="p">(</span><span class="sh">"</span><span class="s">I have an egg!</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="sh">'</span><span class="s">I</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Ġhave</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Ġan</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Ġegg</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">!</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">I have an egg!</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">423</span><span class="p">,</span> <span class="mi">281</span><span class="p">,</span> <span class="mi">5935</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="impact-of-language-on-tokenization">Impact of Language on Tokenization</h3> <p>Text written in English will almost always result in less tokens than the equivalent text in non-English languages. Most western languages, using the Latin alphabet, typically tokenize around words and punctuations. In contrast, logographic systems like Chinese often treat each character as a distinct token, leading to higher token counts.</p> <h4 id="gpt-2-tokenizer-english-vs-chinese-vs-python-code">GPT-2 Tokenizer: English vs Chinese vs Python Code</h4> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">GPT2Tokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">"</span><span class="s">openai-community/gpt2</span><span class="sh">"</span><span class="p">)</span>

<span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">I have an egg</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">423</span><span class="p">,</span> <span class="mi">281</span><span class="p">,</span> <span class="mi">5935</span><span class="p">]</span>

<span class="nf">tokenizer</span><span class="p">(</span><span class="sh">"</span><span class="s">我有个鸡蛋</span><span class="sh">"</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">]</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mi">22755</span><span class="p">,</span> <span class="mi">239</span><span class="p">,</span> <span class="mi">17312</span><span class="p">,</span> <span class="mi">231</span><span class="p">,</span> <span class="mi">10310</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">165</span><span class="p">,</span> <span class="mi">116</span><span class="p">,</span> <span class="mi">94</span><span class="p">,</span> <span class="mi">164</span><span class="p">,</span> <span class="mi">249</span><span class="p">,</span> <span class="mi">233</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>After tokenization (e.g., using GPT-2 tokenizer), the length of non-English sequence is typically longer than the English counter-party. As a result, non-English sentence will be more likely to run out the contextual input that are fed into the model. This is one reason why early versions of GPT are not good at chating in non-English languages.</p> <p>For the code, the individual spaces corresponds to seperate tokens (‘220’). Similar to the non-English sentence, the tokenized code sequence that are fed into the model has a lot of wasteful tokens, making the model harder to learn.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="code"><pre><span class="n">code</span> <span class="o">=</span> <span class="sh">'''</span><span class="s">
class CausalAttention(nn.Module):

    def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False):
        super().__init__()
        self.d_out = d_out
        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)
        self.dropout = nn.Dropout(dropout)  # New
        self.register_buffer(</span><span class="sh">'</span><span class="s">mask</span><span class="sh">'</span><span class="s">, torch.triu(torch.ones(block_size, block_size), diagonal=1))  # New

</span><span class="sh">'''</span>

<span class="nf">len</span><span class="p">(</span><span class="nf">tokenizer</span><span class="p">(</span><span class="n">code</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="mi">255</span>
</pre></td></tr></tbody></table></code></pre></figure> <h3 id="gpt-2-vs-gpt-4-tokenizer">GPT-2 vs GPT-4 tokenizer</h3> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre><span class="n">gpt4_tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="sh">'</span><span class="s">Xenova/gpt-4</span><span class="sh">'</span><span class="p">)</span>

<span class="nf">len</span><span class="p">(</span><span class="nf">gpt4_tokenizer</span><span class="p">(</span><span class="n">code</span><span class="p">)[</span><span class="sh">"</span><span class="s">input_ids</span><span class="sh">"</span><span class="p">])</span>
<span class="o">&gt;</span> <span class="mi">188</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>For the same text, the length of tokenized sequence using GPT-4 tokenizer is shorter than that of using GPT-2 tokenzier (a denser input), indicating the number of tokens in GPT-4 tokenizer (a.k.a. vocabulary size) is larger than that of GPT-2 tokenizer.</p> <p>Compared to GPT-2, GPT-4</p> <ul> <li>can be fed in longer the sequence, i.e., more context can be seen in prediction.</li> <li>the vocab size is larger. The size of embedding table is larger and the cost of softmax operations grows as well. Vocabulary size of GPT-4 vs GPT-2: 100,256 vs 50,257.</li> </ul> <h2 id="build-a-tokenizer">Build a Tokenizer</h2> <h3 id="general-mechanism-of-tokenization-process">General Mechanism of Tokenization Process</h3> <p>A few concept:</p> <ul> <li><a href="https://en.wikipedia.org/wiki/Unicode">unicode</a>: a text encoding standard defined for a large size of characters and scripts. Version 15.1 of the standard defines 149813 characters and 161 scripts used in various ordinary, literary, academic, and technical contexts.</li> <li><a href="https://en.wikipedia.org/wiki/UTF-8">utf-8</a> encoding: it translate unicode code point into one to four bytes。</li> </ul> <p>Why not using unicode as string ids: vocabulary size is too large and is not a stable representation of strings as the standard has been kept chaning.</p> <p>Why not using utf-8: vocabulary size is too small (256). Encoded with utf-8, the sentence length will be notably long and easily consume the context, making the model harder to learn relevant tasks, e.g., next-token prediction.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre><span class="c1"># unicode of character
</span><span class="nf">ord</span><span class="p">(</span><span class="sh">"</span><span class="s">I</span><span class="sh">"</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="mi">73</span>

<span class="p">[</span><span class="nf">ord</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="sh">'</span><span class="s">I have an egg!</span><span class="sh">'</span><span class="p">]</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mi">73</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">104</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">118</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">33</span><span class="p">]</span>

<span class="nf">list</span><span class="p">(</span><span class="sh">'</span><span class="s">I have an egg!</span><span class="sh">'</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-8</span><span class="sh">'</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mi">73</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">104</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">118</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">33</span><span class="p">]</span>


<span class="c1"># utf-16 encoding results in longer and more sparse id list
</span><span class="nf">list</span><span class="p">(</span><span class="sh">'</span><span class="s">I have an egg!</span><span class="sh">'</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf-16</span><span class="sh">'</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mi">255</span><span class="p">,</span> <span class="mi">254</span><span class="p">,</span> <span class="mi">73</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">104</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">118</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">97</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">101</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">103</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></td></tr></tbody></table></code></pre></figure> <p>Based on the disucssion above, an ideal tokenizer is the one that supports a vacaburary with reasonaly large size which can be tuned as a hyperparameter while replying on the utf-8 encodings of strings.</p> <h3 id="byte-level-byte-pair-encoding-bpe">Byte-level Byte Pair Encoding (BPE)</h3> <p>Byte-level BPE is the tokenization algorithm used in GPT-2. The idea is we start from byte sequence with a vocabulary size 256, iteratively find the byte pairs that occur the most, merge as new tokens and append to the vocabulary.</p> <p>To build up a BPE tokenizer, we start by intialize a training process.</p> <p>Note that the code is basically copied from the implementation at <a href="https://github.com/karpathy/minbpe">minbpe</a>.</p> <h4 id="training-merge-by-frequency">Training: Merge by Frequency</h4> <p>As an example below, we start by encoding a sentence in utf-8. Note that after encoding in utf-8, some complex characters have been encoded into multiple bytes (up to four) and therefore the encoded sequence becomes longer.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
</pre></td><td class="code"><pre><span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">💡 Using train_new_from_iterator() on the same corpus won’t result in the exact same vocabulary. This is because when there is a choice of the most frequent pair, we selected the first one encountered, while the 🤗 Tokenizers library selects the first one based on its inner IDs.</span><span class="sh">"</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">length of text in code points</span><span class="sh">'</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">text</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="n">length</span> <span class="n">of</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">code</span> <span class="n">points</span> <span class="mi">277</span>

<span class="c1"># raw bytes
</span><span class="n">tokens</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="sh">'</span><span class="s">utf8</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># list(map(int, tokens))
</span><span class="n">tokens</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">length of text encoded in utf8 tokens </span><span class="sh">'</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>
<span class="o">&gt;</span> <span class="n">length</span> <span class="n">of</span> <span class="n">text</span> <span class="n">encoded</span> <span class="ow">in</span> <span class="n">utf8</span> <span class="n">tokens</span>  <span class="mi">285</span>

<span class="c1"># get the frequency of consecutive byte pairs
</span><span class="k">def</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
  <span class="n">counts</span> <span class="o">=</span> <span class="p">{}</span>

  <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">ids</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">pair</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

  <span class="k">return</span> <span class="n">counts</span>

<span class="n">stats</span> <span class="o">=</span> <span class="nf">get_stats</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="nf">sorted</span><span class="p">(((</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="nf">for </span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">.</span><span class="nf">items</span><span class="p">()),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>

<span class="o">&gt;</span> <span class="p">[(</span><span class="mi">15</span><span class="p">,</span> <span class="p">(</span><span class="mi">101</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">104</span><span class="p">,</span> <span class="mi">101</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">116</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="p">(</span><span class="mi">116</span><span class="p">,</span> <span class="mi">104</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="p">(</span><span class="mi">116</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="p">(</span><span class="mi">115</span><span class="p">,</span> <span class="mi">32</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="mi">110</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">101</span><span class="p">,</span> <span class="mi">114</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">111</span><span class="p">)),</span>
 <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">105</span><span class="p">))]</span>

<span class="c1"># see what is token 101 and 32
</span><span class="nf">chr</span><span class="p">(</span><span class="mi">101</span><span class="p">),</span><span class="nf">chr</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">(</span><span class="sh">'</span><span class="s">e</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></figure> ]]></content><author><name></name></author><category term="LLM"/><category term="pre-training"/><category term="code"/><summary type="html"><![CDATA[a detailed note on llm tokenizer]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://ilampard.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://ilampard.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://ilampard.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">a distill-style blog post</title><link href="https://ilampard.github.io/blog/2021/distill/" rel="alternate" type="text/html" title="a distill-style blog post"/><published>2021-05-22T00:00:00+00:00</published><updated>2021-05-22T00:00:00+00:00</updated><id>https://ilampard.github.io/blog/2021/distill</id><content type="html" xml:base="https://ilampard.github.io/blog/2021/distill/"><![CDATA[<h2 id="equations">Equations</h2> <p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine. You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>. If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p> <p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph. Here is an example:</p> \[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\] <p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> that brought a significant improvement to the loading and rendering speed, which is now <a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p> <hr/> <h2 id="citations">Citations</h2> <p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag. The key attribute is a reference to the id provided in the bibliography. The key attribute can take multiple ids, separated by commas.</p> <p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover). If you have an appendix, a bibliography is automatically created and populated in it.</p> <p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover. However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p> <hr/> <h2 id="footnotes">Footnotes</h2> <p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag. The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p> <hr/> <h2 id="code-blocks">Code Blocks</h2> <p>Syntax highlighting is provided within <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> tags. An example of inline code snippets: <code class="language-plaintext highlighter-rouge">&lt;d-code language="html"&gt;let x = 10;&lt;/d-code&gt;</code>. For larger blocks of code, add a <code class="language-plaintext highlighter-rouge">block</code> attribute:</p> <d-code block="" language="javascript"> var x = 25; function(x) { return x * x; } </d-code> <p><strong>Note:</strong> <code class="language-plaintext highlighter-rouge">&lt;d-code&gt;</code> blocks do not look good in the dark mode. You can always use the default code-highlight using the <code class="language-plaintext highlighter-rouge">highlight</code> liquid tag:</p> <figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="kd">var</span> <span class="nx">x</span> <span class="o">=</span> <span class="mi">25</span><span class="p">;</span>
<span class="kd">function</span><span class="p">(</span><span class="nx">x</span><span class="p">)</span> <span class="p">{</span>
<span class="k">return</span> <span class="nx">x</span> <span class="err">\</span><span class="o">*</span> <span class="nx">x</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure> <hr/> <h2 id="interactive-plots">Interactive Plots</h2> <p>You can add interative plots using plotly + iframes :framed_picture:</p> <div class="l-page"> <iframe src="/assets/plotly/demo.html" frameborder="0" scrolling="no" height="500px" width="100%" style="border: 1px dashed grey;"></iframe> </div> <p>The plot must be generated separately and saved into an HTML file. To generate the plot that you see above, you can use the following code snippet:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span>
<span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
<span class="n">df</span><span class="p">,</span>
<span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span>
<span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span>
<span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">assets/plotly/demo.html</span><span class="sh">'</span><span class="p">)</span></code></pre></figure> <hr/> <h2 id="details-boxes">Details boxes</h2> <p>Details boxes are collapsible boxes which hide additional information from the user. They can be added with the <code class="language-plaintext highlighter-rouge">details</code> liquid tag:</p> <details><summary>Click here to know more</summary> <p>Additional details, where math \(2x - 1\) and <code class="language-plaintext highlighter-rouge">code</code> is rendered correctly.</p> </details> <hr/> <h2 id="layouts">Layouts</h2> <p>The main text column is referred to as the body. It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p> <div class="fake-img l-body"> <p>.l-body</p> </div> <p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p> <div class="fake-img l-page"> <p>.l-page</p> </div> <p>All of these have an outset variant if you want to poke out from the body text a little bit. For instance:</p> <div class="fake-img l-body-outset"> <p>.l-body-outset</p> </div> <div class="fake-img l-page-outset"> <p>.l-page-outset</p> </div> <p>Occasionally you’ll want to use the full browser width. For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>. You can also inset the element a little from the edge of the browser by using the inset variant.</p> <div class="fake-img l-screen"> <p>.l-screen</p> </div> <div class="fake-img l-screen-inset"> <p>.l-screen-inset</p> </div> <p>The final layout is for marginalia, asides, and footnotes. It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code> sized text except on mobile screen sizes.</p> <div class="fake-img l-gutter"> <p>.l-gutter</p> </div> <hr/> <h2 id="other-typography">Other Typography?</h2> <p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p> <p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p> <p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p> <p>Strikethrough uses two tildes. <del>Scratch this.</del></p> <ol> <li>First ordered list item</li> <li>Another item ⋅⋅* Unordered sub-list.</li> <li>Actual numbers don’t matter, just that it’s a number ⋅⋅1. Ordered sub-list</li> <li>And another item.</li> </ol> <p>⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p> <p>⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅ ⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅ ⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)</p> <ul> <li> <p>Unordered list can use asterisks</p> </li> <li> <p>Or minuses</p> </li> <li> <p>Or pluses</p> </li> </ul> <p><a href="https://www.google.com">I’m an inline-style link</a></p> <p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p> <p><a href="https://www.mozilla.org">I’m a reference-style link</a></p> <p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p> <p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p> <p>URLs and URLs in angle brackets will automatically get turned into links. http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes example.com (but not on Github, for example).</p> <p>Some text to show that the reference links can follow later.</p> <p>Here’s our logo (hover to see the title text):</p> <p>Inline-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1"/></p> <p>Reference-style: <img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2"/></p> <p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="k">print</span> <span class="n">s</span>
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting.
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div> <p>Colons can be used to align columns.</p> <table> <thead> <tr> <th>Tables</th> <th style="text-align: center">Are</th> <th style="text-align: right">Cool</th> </tr> </thead> <tbody> <tr> <td>col 3 is</td> <td style="text-align: center">right-aligned</td> <td style="text-align: right">$1600</td> </tr> <tr> <td>col 2 is</td> <td style="text-align: center">centered</td> <td style="text-align: right">$12</td> </tr> <tr> <td>zebra stripes</td> <td style="text-align: center">are neat</td> <td style="text-align: right">$1</td> </tr> </tbody> </table> <p>There must be at least 3 dashes separating each header cell. The outer pipes (|) are optional, and you don’t need to make the raw Markdown line up prettily. You can also use inline Markdown.</p> <table> <thead> <tr> <th>Markdown</th> <th>Less</th> <th>Pretty</th> </tr> </thead> <tbody> <tr> <td><em>Still</em></td> <td><code class="language-plaintext highlighter-rouge">renders</code></td> <td><strong>nicely</strong></td> </tr> <tr> <td>1</td> <td>2</td> <td>3</td> </tr> </tbody> </table> <blockquote> <p>Blockquotes are very handy in email to emulate reply text. This line is part of the same quote.</p> </blockquote> <p>Quote break.</p> <blockquote> <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p> </blockquote> <p>Here’s a line for us to start with.</p> <p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p> <p>This line is also a separate paragraph, but… This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry></feed>